{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n",
      "//anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn \n",
    "import scipy.io as scio\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.signal import welch\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import warnings\n",
    "from GCForest import gcForest\n",
    "import scipy.io as scio\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import skew,kurtosis\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from GCForest import gcForest\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import keras\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from GCForest1 import gcForest\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "from datetime import timedelta\n",
    "import gzip\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import keras\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "import scipy.io as scio\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import skew,kurtosis\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取病人组样本并去均值\n",
    "sick_sample=[]\n",
    "in_dir = 'Downloads/2020数据/'\n",
    "data_paths = [os.path.join(in_dir, f) for f in os.listdir(in_dir)]\n",
    "sick_data_paths = [i for i in data_paths if os.path.isfile(i)]\n",
    "for i in range(len(sick_data_paths)):\n",
    "    if sick_data_paths[i][-1] == 't':\n",
    "        data = pd.read_table(sick_data_paths[i], header=None, engine='python').loc[:,[5,7]].rename(columns={5: 0, 7: 1})\n",
    "        sick_sample.append(data)\n",
    "    \n",
    "# in_dir = 'sEMG_data/sick'\n",
    "# data_paths = [os.path.join(in_dir, f) for f in os.listdir(in_dir)]\n",
    "# sick_data_paths = [i for i in data_paths if os.path.isfile(i)]\n",
    "# for i in range(len(sick_data_paths)):\n",
    "#     data = pd.read_table(sick_data_paths[i], header=None, engine='python').loc[:,[5,7]].rename(columns={5: 0, 7: 1})\n",
    "#     sick_sample.append(data)\n",
    "\n",
    "for i in range(len(sick_sample)):\n",
    "    sick_sample[i] = sick_sample[i] - np.mean(sick_sample[i])\n",
    "    \n",
    "#读取对照组样本并去均值\n",
    "normal_sample=[]\n",
    "in_dir = 'sEMG_data/normal'\n",
    "data_paths = [os.path.join(in_dir, f) for f in os.listdir(in_dir)]\n",
    "normal_data_paths = [i for i in data_paths if os.path.isfile(i)]\n",
    "for i in range(len(normal_data_paths)):\n",
    "    data = pd.read_table(normal_data_paths[i], header=None, engine='python').loc[:,[5,7]].rename(columns={5: 0, 7: 1})\n",
    "    normal_sample.append(data)\n",
    "    \n",
    "for i in range(len(normal_sample)):\n",
    "    normal_sample[i] = normal_sample[i] - np.mean(normal_sample[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取病人组样本并去均值\n",
    "sick_sample=[]\n",
    "in_dir = 'sEMG_data/sick'\n",
    "data_paths = [os.path.join(in_dir, f) for f in os.listdir(in_dir)]\n",
    "sick_data_paths = [i for i in data_paths if os.path.isfile(i)]\n",
    "for i in range(len(sick_data_paths)):\n",
    "    data = pd.read_table(sick_data_paths[i], header=None, engine='python').loc[:,[5,7]].rename(columns={5: 0, 7: 1})\n",
    "    sick_sample.append(data)\n",
    "    \n",
    "for i in range(len(sick_sample)):\n",
    "    sick_sample[i] = sick_sample[i] - np.mean(sick_sample[i])\n",
    "\n",
    "#读取对照组样本并去均值\n",
    "normal_sample=[]\n",
    "in_dir = 'sEMG_data/normal'\n",
    "data_paths = [os.path.join(in_dir, f) for f in os.listdir(in_dir)]\n",
    "normal_data_paths = [i for i in data_paths if os.path.isfile(i)]\n",
    "for i in range(len(normal_data_paths)):\n",
    "    data = pd.read_table(normal_data_paths[i], header=None, engine='python').loc[:,[5,7]].rename(columns={5: 0, 7: 1})\n",
    "    normal_sample.append(data)\n",
    "    \n",
    "for i in range(len(normal_sample)):\n",
    "    normal_sample[i] = normal_sample[i] - np.mean(normal_sample[i])\n",
    "    \n",
    "#sick_sample = sick_sample[:2] + sick_sample[3:6] + sick_sample[8:15] + sick_sample[16:25] + sick_sample[28:32] + sick_sample[33:36] + sick_sample[37:41] + sick_sample[42:-1]\n",
    "#normal_sample = normal_sample[4:7] + normal_sample[8:9] + normal_sample[15:15] + normal_sample[18:-3] + normal_sample[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sick samples:63\n",
      "normal samples:21\n"
     ]
    }
   ],
   "source": [
    "#去噪\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "for i in range(len(sick_sample)):\n",
    "    for j in range(2):\n",
    "        sick_sample[i][j] = butter_bandpass_filter(sick_sample[i][j], lowcut=20, highcut=250, fs=1000, order=8)\n",
    "    \n",
    "for i in range(len(normal_sample)):\n",
    "    for j in range(2):\n",
    "        normal_sample[i][j] = butter_bandpass_filter(normal_sample[i][j], lowcut=20, highcut=250, fs=1000, order=8)\n",
    "        \n",
    "print('sick samples:' + str(len(sick_sample)))\n",
    "print('normal samples:' + str(len(normal_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_1dlbp_hist(data, neighborhood, p):\n",
    "    # Extract the 1d lbp pattern on CPU\n",
    "    res = np.zeros(1 << (2 * neighborhood))\n",
    "    for i in range(neighborhood, len(data) - neighborhood):\n",
    "        left = data[i - neighborhood : i]\n",
    "        right = data[i + 1 : i + neighborhood + 1]\n",
    "        both = np.r_[left, right]\n",
    "        res[np.sum(p [both >= data[i]])] += 1\n",
    "    return res\n",
    "\n",
    "def extract_1dlbp(data, neighborhood, p):\n",
    "    # Extract the 1d lbp pattern on CPU\n",
    "    res = np.zeros(len(data) - 2 * neighborhood)\n",
    "    for i in range(neighborhood, len(data) - neighborhood):\n",
    "        left = data[i - neighborhood : i]\n",
    "        right = data[i + 1 : i + neighborhood + 1]\n",
    "        both = np.r_[left, right]\n",
    "        res[i-neighborhood] = np.sum(p [both >= data[i]])\n",
    "    return res\n",
    "\n",
    "def get_psd_values(y_values, feq):\n",
    "    f_values, psd_values = welch(y_values, fs=feq)\n",
    "    return f_values, psd_values\n",
    "\n",
    "def plot_hist(ax,lbp,width):\n",
    "    myDictionary = dict(zip(np.arange(len(lbp)),lbp))\n",
    "    ax.bar(myDictionary.keys(), myDictionary.values(), width=width, color='g')\n",
    "\n",
    "neighborhood = 4\n",
    "p = 1 << np.array(range(0, 2 * neighborhood), dtype='int32')\n",
    "\n",
    "def RMS(data):\n",
    "    return np.sqrt(np.mean(data**2))\n",
    "\n",
    "def MAV(data):\n",
    "    return np.mean(np.abs(data))\n",
    "\n",
    "def IEMG(data):\n",
    "    return np.sum(np.abs(data))\n",
    "\n",
    "def ZC(data):\n",
    "    return np.sum(data[k] * data[k+1] < 0 for k in range(len(data) - 1))\n",
    "\n",
    "def SSC(data):\n",
    "    return np.sum((data[k] < data[k+1] and data[k] < data[k-1]) or (data[k] > data[k+1] and data[k] > data[k-1]) for k in range(1, len(data) - 1))\n",
    "\n",
    "def WL(data):\n",
    "    return np.sum(abs(data[k] - data[k+1]) for k in range(len(data) - 1))\n",
    "\n",
    "def SKEW(data):\n",
    "    return skew(data)\n",
    "\n",
    "def KURTOSIS(data):\n",
    "    return kurtosis(data)\n",
    "\n",
    "def VAR(data):\n",
    "    return np.mean(np.sum(data**2))\n",
    "\n",
    "def MPF(f, psd):\n",
    "    return np.dot(f, psd) / np.sum(psd)\n",
    "\n",
    "def MF(f, psd):\n",
    "    sum0 = np.sum(psd)\n",
    "    now = 0\n",
    "    for i in range(len(psd)):\n",
    "        now += psd[i]\n",
    "        if now >= sum0/2:\n",
    "            return f[i]\n",
    "        \n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = len(sick_sample) + len(normal_sample)\n",
    "num_psd_feature = 258\n",
    "num_lbp_feature = 256 * 2\n",
    "num_classes = 6\n",
    "feature = np.zeros((num_sample, num_psd_feature + num_lbp_feature))\n",
    "\n",
    "for i in range(len(sick_sample)):\n",
    "    cur_feature = get_psd_values(100*sick_sample[i][0][:-2],1000)[1]\n",
    "    cur_feature = np.append(cur_feature, get_psd_values(100*sick_sample[i][1][:-2],1000)[1])\n",
    "    cur_feature = np.append(cur_feature, extract_1dlbp_hist(sick_sample[i][0][:-2], neighborhood, p))\n",
    "    cur_feature = np.append(cur_feature, extract_1dlbp_hist(sick_sample[i][1][:-2], neighborhood, p))\n",
    "    \n",
    "    feature[i,:] = np.ravel(cur_feature)\n",
    "\n",
    "for i in range(len(normal_sample)):\n",
    "    cur_feature = get_psd_values(100*normal_sample[i][0][:-2],1000)[1]\n",
    "    cur_feature = np.append(cur_feature, get_psd_values(100*normal_sample[i][1][:-2],1000)[1])\n",
    "    cur_feature = np.append(cur_feature, extract_1dlbp_hist(normal_sample[i][0][:-2], neighborhood, p))\n",
    "    cur_feature = np.append(cur_feature, extract_1dlbp_hist(normal_sample[i][1][:-2], neighborhood, p))\n",
    "    \n",
    "    feature[len(sick_sample) + i,:] = np.ravel(cur_feature)\n",
    "    \n",
    "target = np.ones((num_sample,1)).flatten()\n",
    "target[len(sick_sample):num_sample] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = len(sick_sample) + len(normal_sample)\n",
    "num_classes = 6\n",
    "feature = np.zeros((num_sample, 23))\n",
    "\n",
    "for i in range(len(sick_sample)):\n",
    "    \n",
    "    data = sick_sample[i][0].values\n",
    "    cur_feature=np.zeros(1)\n",
    "    cur_feature=np.append(cur_feature,RMS(data))\n",
    "    cur_feature=np.append(cur_feature,MAV(data))\n",
    "    cur_feature=np.append(cur_feature,IEMG(data))\n",
    "    cur_feature=np.append(cur_feature,ZC(data))\n",
    "    cur_feature=np.append(cur_feature,SSC(data))\n",
    "    cur_feature=np.append(cur_feature,WL(data))\n",
    "    cur_feature=np.append(cur_feature,SKEW(data))\n",
    "    cur_feature=np.append(cur_feature,KURTOSIS(data))\n",
    "    cur_feature=np.append(cur_feature,VAR(data))\n",
    "    f_values, psd_values = get_psd_values(data, 1000)\n",
    "    cur_feature=np.append(cur_feature,MPF(f_values, psd_values))\n",
    "    cur_feature=np.append(cur_feature,MF(f_values, psd_values))\n",
    "\n",
    "    data = sick_sample[i][1].values\n",
    "    cur_feature=np.append(cur_feature,RMS(data))\n",
    "    cur_feature=np.append(cur_feature,MAV(data))\n",
    "    cur_feature=np.append(cur_feature,IEMG(data))\n",
    "    cur_feature=np.append(cur_feature,ZC(data))\n",
    "    cur_feature=np.append(cur_feature,SSC(data))\n",
    "    cur_feature=np.append(cur_feature,WL(data))\n",
    "    cur_feature=np.append(cur_feature,SKEW(data))\n",
    "    cur_feature=np.append(cur_feature,KURTOSIS(data))\n",
    "    cur_feature=np.append(cur_feature,VAR(data))\n",
    "    f_values, psd_values = get_psd_values(data, 1000)\n",
    "    cur_feature=np.append(cur_feature,MPF(f_values, psd_values))\n",
    "    cur_feature=np.append(cur_feature,MF(f_values, psd_values))\n",
    "    \n",
    "    feature[i,:] = np.ravel(cur_feature)\n",
    "\n",
    "for i in range(len(normal_sample)):\n",
    "    \n",
    "    data = normal_sample[i][0].values\n",
    "    cur_feature=np.zeros(1)\n",
    "    cur_feature=np.append(cur_feature,RMS(data))\n",
    "    cur_feature=np.append(cur_feature,MAV(data))\n",
    "    cur_feature=np.append(cur_feature,IEMG(data))\n",
    "    cur_feature=np.append(cur_feature,ZC(data))\n",
    "    cur_feature=np.append(cur_feature,SSC(data))\n",
    "    cur_feature=np.append(cur_feature,WL(data))\n",
    "    cur_feature=np.append(cur_feature,SKEW(data))\n",
    "    cur_feature=np.append(cur_feature,KURTOSIS(data))\n",
    "    cur_feature=np.append(cur_feature,VAR(data))\n",
    "    f_values, psd_values = get_psd_values(data, 1000)\n",
    "    cur_feature=np.append(cur_feature,MPF(f_values, psd_values))\n",
    "    cur_feature=np.append(cur_feature,MF(f_values, psd_values))\n",
    "\n",
    "    data = normal_sample[i][1].values\n",
    "    cur_feature=np.append(cur_feature,RMS(data))\n",
    "    cur_feature=np.append(cur_feature,MAV(data))\n",
    "    cur_feature=np.append(cur_feature,IEMG(data))\n",
    "    cur_feature=np.append(cur_feature,ZC(data))\n",
    "    cur_feature=np.append(cur_feature,SSC(data))\n",
    "    cur_feature=np.append(cur_feature,WL(data))\n",
    "    cur_feature=np.append(cur_feature,SKEW(data))\n",
    "    cur_feature=np.append(cur_feature,KURTOSIS(data))\n",
    "    cur_feature=np.append(cur_feature,VAR(data))\n",
    "    f_values, psd_values = get_psd_values(data, 1000)\n",
    "    cur_feature=np.append(cur_feature,MPF(f_values, psd_values))\n",
    "    cur_feature=np.append(cur_feature,MF(f_values, psd_values))\n",
    "    \n",
    "    feature[len(sick_sample) + i,:] = np.ravel(cur_feature)\n",
    "    \n",
    "target = np.ones((num_sample,1)).flatten()\n",
    "target[len(sick_sample):num_sample] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.8823529411764706\n",
      "gcf accuracy:0.7647058823529411\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.8823529411764706\n",
      "SVM accuracy : 0.7647058823529411\n",
      "MLP accuracy : 0.23529411764705882\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8235294117647058\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.6470588235294118\n",
      "SVM accuracy : 0.7647058823529411\n",
      "MLP accuracy : 0.23529411764705882\n",
      "rf accuracy : 0.7647058823529411\n",
      "gcf accuracy:0.7647058823529411\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "KNN accuracy : 0.7647058823529411\n",
      "SVM accuracy : 0.7647058823529411\n",
      "MLP accuracy : 0.23529411764705882\n",
      "rf accuracy : 0.8823529411764706\n",
      "gcf accuracy:0.8823529411764706\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "KNN accuracy : 0.8125\n",
      "SVM accuracy : 0.75\n",
      "MLP accuracy : 0.25\n",
      "rf accuracy : 0.8125\n",
      "gcf accuracy:0.8125\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.6470588235294118\n",
      "MLP accuracy : 0.35294117647058826\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8823529411764706\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.7058823529411765\n",
      "SVM accuracy : 0.6470588235294118\n",
      "MLP accuracy : 0.35294117647058826\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8235294117647058\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.6470588235294118\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.7058823529411765\n",
      "gcf accuracy:0.7058823529411765\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.9411764705882353\n",
      "MLP accuracy : 0.058823529411764705\n",
      "rf accuracy : 1.0\n",
      "gcf accuracy:0.9411764705882353\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.8125\n",
      "SVM accuracy : 0.8125\n",
      "MLP accuracy : 0.1875\n",
      "rf accuracy : 0.75\n",
      "gcf accuracy:0.875\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.7647058823529411\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.6470588235294118\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.8823529411764706\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8823529411764706\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 1.0\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.6470588235294118\n",
      "SVM accuracy : 0.8823529411764706\n",
      "MLP accuracy : 0.11764705882352941\n",
      "rf accuracy : 0.7647058823529411\n",
      "gcf accuracy:0.7647058823529411\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8235294117647058\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.75\n",
      "SVM accuracy : 0.75\n",
      "MLP accuracy : 0.25\n",
      "rf accuracy : 0.875\n",
      "gcf accuracy:0.875\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.7647058823529411\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9285714285714286\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9285714285714286\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.9285714285714286\n",
      "KNN accuracy : 0.7058823529411765\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8235294117647058\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.6470588235294118\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.7058823529411765\n",
      "gcf accuracy:0.5882352941176471\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.7058823529411765\n",
      "MLP accuracy : 0.29411764705882354\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8235294117647058\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7142857142857143\n",
      "KNN accuracy : 0.5625\n",
      "SVM accuracy : 0.9375\n",
      "MLP accuracy : 0.0625\n",
      "rf accuracy : 0.8125\n",
      "gcf accuracy:0.8125\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "KNN accuracy : 0.7058823529411765\n",
      "SVM accuracy : 0.6470588235294118\n",
      "MLP accuracy : 0.35294117647058826\n",
      "rf accuracy : 0.7058823529411765\n",
      "gcf accuracy:0.7058823529411765\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.8235294117647058\n",
      "MLP accuracy : 0.17647058823529413\n",
      "rf accuracy : 0.8235294117647058\n",
      "gcf accuracy:0.8235294117647058\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.7058823529411765\n",
      "SVM accuracy : 0.6470588235294118\n",
      "MLP accuracy : 0.35294117647058826\n",
      "rf accuracy : 0.8823529411764706\n",
      "gcf accuracy:0.8823529411764706\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.6428571428571429\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.6428571428571429\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.5714285714285714\n",
      "KNN accuracy : 0.8235294117647058\n",
      "SVM accuracy : 0.7647058823529411\n",
      "MLP accuracy : 0.23529411764705882\n",
      "rf accuracy : 0.8823529411764706\n",
      "gcf accuracy:0.8823529411764706\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.7857142857142857\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "Adding/Training Layer, n_layer=4\n",
      "Layer validation accuracy = 0.8571428571428571\n",
      "KNN accuracy : 0.75\n",
      "SVM accuracy : 0.875\n",
      "MLP accuracy : 0.125\n",
      "rf accuracy : 0.8125\n",
      "gcf accuracy:0.8125\n",
      "knn mean accuracy:0.7592647058823531\n",
      "svm mean accuracy:0.7508823529411766\n",
      "mlp mean accuracy:0.24911764705882355\n",
      "rf mean accuracy:0.818970588235294\n",
      "gcf mean accuracy:0.8075\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "gcf_accuracy = []\n",
    "result = []\n",
    "knn_accuracy = []\n",
    "svm_accuracy = []\n",
    "mlp_accuracy = []\n",
    "feature = np.nan_to_num(feature)\n",
    "\n",
    "for i in range(5):\n",
    "    kf = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, test_index in kf.split(feature):\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(np.nan_to_num(feature), target, stratify = target, test_size=0.2)\n",
    "        X_train, X_test, y_train, y_test = feature[train_index,:], feature[test_index,:], target[train_index], target[test_index]\n",
    "    \n",
    "        rf = RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "        rf.fit(X_train,y_train)\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        accuracy = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        result.append(accuracy)\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=1)\n",
    "        knn.fit(X_train,y_train)\n",
    "        predictions = knn.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        knn_res = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        knn_accuracy.append(knn_res)\n",
    "\n",
    "\n",
    "        svm = SVC(kernel='rbf')\n",
    "        svm.fit(X_train,y_train)\n",
    "        predictions = svm.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        svm_res = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        svm_accuracy.append(svm_res)\n",
    "\n",
    "        \n",
    "        mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(64, 32), random_state=1)\n",
    "        mlp.fit(X_train,y_train)\n",
    "        predictions = mlp.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        mlp_res = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        mlp_accuracy.append(mlp_res)\n",
    "\n",
    "        gcf = gcForest(shape_1X=X_train.shape[1], window=[20], stride=20, tolerance=0, n_mgsRF=-1,\n",
    "                           n_cascadeRF=1, n_cascadeRFtree=200, min_cascade_layer=2, max_cascade_layer=8, version=0)\n",
    "        gcf.fit(X_train, y_train)\n",
    "        pred_X = gcf.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        res = accuracy_score(y_true=y_test, y_pred=pred_X)\n",
    "        \n",
    "        print('KNN accuracy : {}'.format(knn_res))\n",
    "        print('SVM accuracy : {}'.format(svm_res))\n",
    "        print('MLP accuracy : {}'.format(mlp_res))\n",
    "        print('rf accuracy : {}'.format(accuracy))\n",
    "        print('gcf accuracy:'+str(res))\n",
    "        gcf_accuracy.append(res)\n",
    "\n",
    "print('knn mean accuracy:'+str(np.mean(knn_accuracy)))\n",
    "print('svm mean accuracy:'+str(np.mean(svm_accuracy)))\n",
    "print('mlp mean accuracy:'+str(np.mean(mlp_accuracy)))\n",
    "print('rf mean accuracy:'+str(np.mean(result)))\n",
    "print('gcf mean accuracy:'+str(np.mean(gcf_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcf_accuracy = []\n",
    "result = []\n",
    "knn_accuracy = []\n",
    "svm_accuracy = []\n",
    "mlp_accuracy = []\n",
    "feature = np.nan_to_num(feature)\n",
    "\n",
    "for i in range(5):\n",
    "    kf = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, test_index in kf.split(feature):\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(np.nan_to_num(feature), target, stratify = target, test_size=0.2)\n",
    "        X_train, X_test, y_train, y_test = feature[train_index,:], feature[test_index,:], target[train_index], target[test_index]\n",
    "    \n",
    "        rf = RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "        rf.fit(X_train,y_train)\n",
    "        predictions = rf.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        accuracy = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        result.append(accuracy)\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=1)\n",
    "        knn.fit(X_train,y_train)\n",
    "        predictions = knn.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        knn_res = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        knn_accuracy.append(knn_res)\n",
    "\n",
    "\n",
    "        svm = SVC(kernel='rbf')\n",
    "        svm.fit(X_train,y_train)\n",
    "        predictions = svm.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        svm_res = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        svm_accuracy.append(svm_res)\n",
    "\n",
    "        \n",
    "        mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(64, 32), random_state=1)\n",
    "        mlp.fit(X_train,y_train)\n",
    "        predictions = mlp.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        mlp_res = accuracy_score(y_true=y_test, y_pred=predictions)\n",
    "        mlp_accuracy.append(mlp_res)\n",
    "\n",
    "        gcf = gcForest(shape_1X=X_train.shape[1], window=[20], stride=20, tolerance=0, n_mgsRF=-1,\n",
    "                           n_cascadeRF=1, n_cascadeRFtree=200, min_cascade_layer=2, max_cascade_layer=8, version=0)\n",
    "        gcf.fit(X_train, y_train)\n",
    "        pred_X = gcf.predict(X_test)\n",
    "\n",
    "        # evaluating accuracy\n",
    "        res = accuracy_score(y_true=y_test, y_pred=pred_X)\n",
    "        \n",
    "        print('KNN accuracy : {}'.format(knn_res))\n",
    "        print('SVM accuracy : {}'.format(svm_res))\n",
    "        print('MLP accuracy : {}'.format(mlp_res))\n",
    "        print('rf accuracy : {}'.format(accuracy))\n",
    "        print('gcf accuracy:'+str(res))\n",
    "        gcf_accuracy.append(res)\n",
    "\n",
    "print('knn mean accuracy:'+str(np.mean(knn_accuracy)))\n",
    "print('svm mean accuracy:'+str(np.mean(svm_accuracy)))\n",
    "print('mlp mean accuracy:'+str(np.mean(mlp_accuracy)))\n",
    "print('rf mean accuracy:'+str(np.mean(result)))\n",
    "print('gcf mean accuracy:'+str(np.mean(gcf_accuracy)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
